{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 4\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names\n",
    "----\n",
    "Names: __YOUR NAMES HERE__ (Write these in every notebook you submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Neural Networks (20 points)\n",
    "----\n",
    "\n",
    "Next, we'll train a feedforward neural net to work with this data. You'll train one neural net which takes the same input as your Logistic Regression model - a sparse vector representing documents as bags of words.\n",
    "\n",
    "Take a look at these videos to understand forward and backward propagation in neural networks - \n",
    "* https://www.youtube.com/watch?v=HHbjpDHcJVw\n",
    "* https://youtu.be/-Lavz_I4l2U?si=zi20DB3qKPLMEPt1\n",
    "\n",
    "You will be implementing **binarized** (presence/absence of word) and **multinomial** (counts of word) BoW representations of your data\n",
    "  \n",
    "**10 points in Task 5 will be allocated for all 9 graphs (including the one generated here in Task 4 for Neural Networks) being:**\n",
    "- Legible\n",
    "- Present below\n",
    "- Properly labeled\n",
    "     - x and y axes labeled\n",
    "     - Legend for accuracy measures plotted\n",
    "     - Plot Title with which model and run number the graph represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentiment_utils as sutils\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# you can experiment with having some Dropout layers if you'd like to\n",
    "# this is not required\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# if you want to use this again\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\"\n",
    "\n",
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "train_tups = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "dev_tups = sutils.generate_tuples_from_file(DEV_FILE)\n",
    "\n",
    "# you may use either your sparse vectors or sklearn's CountVectorizer's sparse vectors\n",
    "# you will experiment with multinomial and binarized representations later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward neural network model\n",
    "# that takes a sparse BoW representation of the data as input\n",
    "# and makes a binary classification of positive/negative sentiment as output\n",
    "# you may use any number of hidden layers >= 1 and any number of units in each hidden layer (we recommend between 50-200)\n",
    "# you may use any activation function on the hidden layers \n",
    "# you should use a sigmoid activation function on the output layer\n",
    "# you should use binary cross-entropy as your loss function\n",
    "# sgd is an appropriate optimizer for this task\n",
    "# you should report accuracy as your metric\n",
    "# you may add Dropout layers if you'd like to\n",
    "\n",
    "# create/compile your model in this cell\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "# put in an output layer\n",
    "\n",
    "\n",
    "model.summary()\n",
    "# call compile here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many trainable parameters does your model have? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your model\n",
    "\n",
    "\n",
    "# Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})\n",
    "# indicates you should change a list into a numpy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction on the dev set\n",
    "# then make a classification decision based on that prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model.evaluate function to report the loss and accuracy on the dev set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">__Expected Behavior__ </span>\n",
    "\n",
    "**Neural Networks**:\n",
    "Neural networks initialize their weights randomly and learn through iterative stochastic optimization, which introduces non-determinism by design. Even with the same data and parameters, different runs may lead to slightly different learned weights and therefore different graphs. In this case, variation between your three runs is expected and desired, it shows how the model’s training process can vary due to randomness.\n",
    "\n",
    "<span style=\"color: red;\">__Note on Training Data Increments__ </span>\n",
    "\n",
    "When varying the amount of training data, choose increments that are meaningful and reasonable, you should be able to observe clear trends without making the experiment unnecessarily long. You may increment the training data percentage by **5%**, **10%** or **20%**.\n",
    "\n",
    "**Make sure that one of your experiments includes 10% of the training data, as you will need this result to answer a question in Task 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the same graph as with NB and LR, with your neural network model instead!\n",
    "# make sure to re-create your model each time you train it — you don't want to start with\n",
    "# an already trained network!\n",
    "\n",
    "# you should experiment with different numbers of epochs to see how performance varies\n",
    "# you need not create an experiment that takes > 10 min to run (gradescope will run out of computing resources and give you a 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the f1 scores for your model with the following settings, using the same number of epochs to train in both cases:\n",
    "- number of epochs used: __YOUR ANSWER HERE__\n",
    "- multinomial features: __YOUR ANSWER HERE__ \n",
    "- binarized features: __YOUR ANSWER HERE__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
