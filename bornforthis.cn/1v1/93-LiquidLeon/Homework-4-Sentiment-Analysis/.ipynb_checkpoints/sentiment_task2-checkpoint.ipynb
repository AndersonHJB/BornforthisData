{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 2\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names \n",
    "----\n",
    "Names: __YOUR NAMES HERE__ (Write these in every notebook you submit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Train a Naive Bayes Model (30 points)\n",
    "----\n",
    "\n",
    "Using `nltk`'s `NaiveBayesClassifier` class, train a Naive Bayes classifier using a Bag of Words as features.\n",
    "\n",
    "You will be implementing **binarized** (presence/absence of word) and **multinomial** (counts of word) BoW representations of your data\n",
    "\n",
    "Learn more about Naive Bayes here: https://www.nltk.org/_modules/nltk/classify/naivebayes.html \n",
    "\n",
    "Naive Bayes classifiers use Bayesâ€™ theorem for predictions. Naive Bayes can be a good baseline for NLP applications in particular. You can use it as a baseline for your project!\n",
    "\n",
    "**\n",
    "\n",
    "**10 points in Task 5 will be allocated for all 9 graphs (including the one generated here in Task 4 for Naive Bayes Classifier) being:**\n",
    "- Legible\n",
    "- Present below\n",
    "- Properly labeled\n",
    "     - x and y axes labeled\n",
    "     - Legend for accuracy measures plotted\n",
    "     - Plot Title with which model and run number the graph represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our utility functions\n",
    "# RESTART your jupyter notebook kernel if you make changes to this file\n",
    "import sentiment_utils as sutils\n",
    "\n",
    "# nltk for Naive Bayes and metrics\n",
    "import nltk\n",
    "import nltk.classify.util\n",
    "from nltk.metrics.scores import (precision, recall, f_measure, accuracy)\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# some potentially helpful data structures from collections\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# so that we can make plots\n",
    "import matplotlib.pyplot as plt\n",
    "# if you want to use seaborn to make plots\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "train_tups = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "dev_tups = sutils.generate_tuples_from_file(DEV_FILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a sentiment classifier using NLTK's NaiveBayesClassifier and \n",
    "# a bag of words as features\n",
    "# take a look at the function in lecture notebook 7 (feel free to copy + paste that function)\n",
    "# the nltk classifier expects a dictionary of features as input where the key is the feature name\n",
    "# and the value is the feature value\n",
    "\n",
    "# need to return a dict to work with the NLTK classifier\n",
    "# Possible problem for students: evaluate the difference \n",
    "# between using binarized features and using counts (non binarized features)\n",
    "def word_feats(PARAMETERS ARE UP TO YOU) -> dict:    \n",
    "    # STUDENTS IMPLEMENT\n",
    "    pass         \n",
    "\n",
    "\n",
    "# set up & train a sentiment classifier using NLTK's NaiveBayesClassifier and\n",
    "# classify the first example in the dev set as an example\n",
    "# make sure your output is well-labeled\n",
    "\n",
    "\n",
    "\n",
    "# test to make sure that you can train the classifier and use it to classify a new example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">__Expected Behavior__ </span>\n",
    "\n",
    "**Naive Bayes**:\n",
    "Naive Bayes relies on word counts or feature frequencies to compute probabilities. Since it does not involve random initialization, it is a deterministic algorithm: meaning it will always produce identical results given the same data and preprocessing steps. So, if your Naive Bayes graphs are identical across runs, this is expected and completely fine!\n",
    "\n",
    "<span style=\"color: red;\">__Note on Training Data Increments__ </span>\n",
    "\n",
    "When varying the amount of training data, choose increments that are meaningful and reasonable, you should be able to observe clear trends without making the experiment unnecessarily long. You may increment the training data percentage by **5%**, **10%** or **20%**.\n",
    "\n",
    "**Make sure that one of your experiments includes 10% of the training data, as you will need this result to answer a question in Task 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the provided dev set, evaluate your model with precision, recall, and f1 score as well as accuracy\n",
    "# You may use nltk's implemented `precision`, `recall`, `f_measure`, and `accuracy` functions\n",
    "# (make sure to look at the documentation for these functions!)\n",
    "# you will be creating a similar graph for logistic regression and neural nets, so make sure\n",
    "# you use functions wisely so that you do not have excessive repeated code\n",
    "# write any helper functions you need in sentiment_utils.py (functions that you'll use in your other notebooks as well)\n",
    "\n",
    "\n",
    "# create a graph of your classifier's performance on the dev set as a function of the amount of training data\n",
    "# the x-axis should be the amount of training data (as a percentage of the total training data)\n",
    "# NOTE : make sure one of your experiments uses 10% of the data, you will need this to answer the first question in task 5\n",
    "# the y-axis should be the performance of the classifier on the dev set\n",
    "# the graph should have 4 lines, one for each of precision, recall, f1, and accuracy\n",
    "# the graph should have a legend, title, and axis labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your model using both a __binarized__ (bag of words representation where we put 1 [true] if the word is there and 0 [false] otherwise) and a __multinomial__ (bag of words representation where we put the count of the word if the word occurs, and 0 otherwise). Use whichever one gives you a better final f1 score on the dev set to produce your graphs.\n",
    "\n",
    "- f1 score binarized: __YOUR ANSWER HERE__\n",
    "- f1 score multinomial: __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
