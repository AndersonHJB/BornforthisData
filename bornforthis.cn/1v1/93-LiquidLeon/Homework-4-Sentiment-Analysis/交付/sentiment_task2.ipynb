{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 2\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names \n",
    "----\n",
    "Names: __YOUR NAMES HERE__ (Write these in every notebook you submit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Train a Naive Bayes Model (30 points)\n",
    "----\n",
    "\n",
    "Using `nltk`'s `NaiveBayesClassifier` class, train a Naive Bayes classifier using a Bag of Words as features.\n",
    "\n",
    "You will be implementing **binarized** (presence/absence of word) and **multinomial** (counts of word) BoW representations of your data\n",
    "\n",
    "Learn more about Naive Bayes here: https://www.nltk.org/_modules/nltk/classify/naivebayes.html \n",
    "\n",
    "Naive Bayes classifiers use Bayes’ theorem for predictions. Naive Bayes can be a good baseline for NLP applications in particular. You can use it as a baseline for your project!\n",
    "\n",
    "**\n",
    "\n",
    "**10 points in Task 5 will be allocated for all 9 graphs (including the one generated here in Task 4 for Naive Bayes Classifier) being:**\n",
    "- Legible\n",
    "- Present below\n",
    "- Properly labeled\n",
    "     - x and y axes labeled\n",
    "     - Legend for accuracy measures plotted\n",
    "     - Plot Title with which model and run number the graph represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our utility functions\n",
    "# RESTART your jupyter notebook kernel if you make changes to this file\n",
    "import sentiment_utils as sutils\n",
    "\n",
    "# nltk for Naive Bayes and metrics\n",
    "import nltk\n",
    "import nltk.classify.util\n",
    "from nltk.metrics.scores import (precision, recall, f_measure, accuracy)\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# some potentially helpful data structures from collections\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# so that we can make plots\n",
    "import matplotlib.pyplot as plt\n",
    "# if you want to use seaborn to make plots\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "# train_tups = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "# dev_tups = sutils.generate_tuples_from_file(DEV_FILE)\n",
    "\n",
    "# Load tokenized data\n",
    "train_X, train_y = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "dev_X,   dev_y   = sutils.generate_tuples_from_file(DEV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final F1 (binarized)  : 0.7761\n",
      "Final F1 (multinomial): 0.7813\n"
     ]
    }
   ],
   "source": [
    "# set up a sentiment classifier using NLTK's NaiveBayesClassifier and \n",
    "# a bag of words as features\n",
    "# take a look at the function in lecture notebook 7 (feel free to copy + paste that function)\n",
    "# the nltk classifier expects a dictionary of features as input where the key is the feature name\n",
    "# and the value is the feature value\n",
    "\n",
    "# need to return a dict to work with the NLTK classifier\n",
    "# Possible problem for students: evaluate the difference \n",
    "# between using binarized features and using counts (non binarized features)\n",
    "# (Optional) build a vocab from training set; you may use it to filter features if desired\n",
    "vocab = set(sutils.create_index(train_X, min_freq=1))\n",
    "\n",
    "# === Assignment-style feature function ===\n",
    "def word_feats(tokens, binary: bool = False, use_train_vocab: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    将一篇文档（分词列表）转为 NB 可用的特征字典。\n",
    "    参数:\n",
    "        tokens: List[str] 该样本的分词序列\n",
    "        binary: True -> 二值特征; False -> 多项式（计数）特征\n",
    "        use_train_vocab: 若为 True，则仅保留出现在训练词表中的词（避免引入未见词）\n",
    "    返回:\n",
    "        dict[str, int]  特征名->特征值\n",
    "    \"\"\"\n",
    "    if use_train_vocab:\n",
    "        cnt = Counter(t.lower() for t in tokens if t.lower() in vocab)\n",
    "    else:\n",
    "        cnt = Counter(t.lower() for t in tokens)\n",
    "    if binary:\n",
    "        return {w: 1 for w in cnt.keys()}\n",
    "    else:\n",
    "        return dict(cnt)\n",
    "\n",
    "def build_instances(X_tok, y, binary: bool, use_train_vocab: bool = True):\n",
    "    feats = [word_feats(toks, binary=binary, use_train_vocab=use_train_vocab) for toks in X_tok]\n",
    "    return list(zip(feats, y))\n",
    "\n",
    "def train_eval_nb(percent: int, binary: bool, seed: int = 0, use_train_vocab: bool = True):\n",
    "    # sample percent% of training data deterministically\n",
    "    sub_X, sub_y = sutils.take_percent(train_X, train_y, percent, shuffle=True, seed=seed)\n",
    "    train_set = build_instances(sub_X, sub_y, binary=binary, use_train_vocab=use_train_vocab)\n",
    "    dev_set   = build_instances(dev_X, dev_y, binary=binary, use_train_vocab=use_train_vocab)\n",
    "\n",
    "    # Train NB\n",
    "    clf = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    # Predict on dev\n",
    "    preds = [clf.classify(feats) for feats, _ in dev_set]\n",
    "    preds = [int(p) for p in preds]\n",
    "\n",
    "    # Metrics\n",
    "    prec, rec, f1, acc = sutils.get_prfa(dev_y, preds, verbose=False)\n",
    "    return prec, rec, f1, acc\n",
    "\n",
    "def plot_runs(binary: bool, run_id: int, percents=None, save_as=None, use_train_vocab: bool = True):\n",
    "    if percents is None:\n",
    "        percents = [10, 20, 40, 60, 80, 100]\n",
    "    title = f\"Naive Bayes ({'Binarized' if binary else 'Multinomial'}) — Run {run_id}\"\n",
    "    curves = sutils.create_training_graph(\n",
    "        metrics_fun=lambda p: train_eval_nb(p, binary=binary, seed=run_id, use_train_vocab=use_train_vocab),\n",
    "        percents=percents,\n",
    "        title=title,\n",
    "        savepath=save_as\n",
    "    )\n",
    "    return curves   \n",
    "\n",
    "\n",
    "# set up & train a sentiment classifier using NLTK's NaiveBayesClassifier and\n",
    "# classify the first example in the dev set as an example\n",
    "# make sure your output is well-labeled\n",
    "\n",
    "\n",
    "\n",
    "# test to make sure that you can train the classifier and use it to classify a new example\n",
    "if __name__ == \"__main__\":\n",
    "    # Produce both variants and save graphs (three runs each)\n",
    "    for run in (1,2,3):\n",
    "        plot_runs(binary=False, run_id=run, save_as=f\"Naive_Bayes_multinomial_run{run}.png\")\n",
    "        plot_runs(binary=True,  run_id=run, save_as=f\"Naive_Bayes_binarized_run{run}.png\")\n",
    "\n",
    "    # Quick comparison on full dev set\n",
    "    _,_,f1_bin,_   = train_eval_nb(100, binary=True,  seed=1)\n",
    "    _,_,f1_multi,_ = train_eval_nb(100, binary=False, seed=1)\n",
    "    print(f\"Final F1 (binarized)  : {f1_bin:.4f}\")\n",
    "    print(f\"Final F1 (multinomial): {f1_multi:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">__Expected Behavior__ </span>\n",
    "\n",
    "**Naive Bayes**:\n",
    "Naive Bayes relies on word counts or feature frequencies to compute probabilities. Since it does not involve random initialization, it is a deterministic algorithm: meaning it will always produce identical results given the same data and preprocessing steps. So, if your Naive Bayes graphs are identical across runs, this is expected and completely fine!\n",
    "\n",
    "<span style=\"color: red;\">__Note on Training Data Increments__ </span>\n",
    "\n",
    "When varying the amount of training data, choose increments that are meaningful and reasonable, you should be able to observe clear trends without making the experiment unnecessarily long. You may increment the training data percentage by **5%**, **10%** or **20%**.\n",
    "\n",
    "**Make sure that one of your experiments includes 10% of the training data, as you will need this result to answer a question in Task 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes 使用 multinomial 特征时表现略好（F1=0.7813 vs 0.7761）。\n",
    "# Using the provided dev set, evaluate your model with precision, recall, and f1 score as well as accuracy\n",
    "# You may use nltk's implemented `precision`, `recall`, `f_measure`, and `accuracy` functions\n",
    "# (make sure to look at the documentation for these functions!)\n",
    "# you will be creating a similar graph for logistic regression and neural nets, so make sure\n",
    "# you use functions wisely so that you do not have excessive repeated code\n",
    "# write any helper functions you need in sentiment_utils.py (functions that you'll use in your other notebooks as well)\n",
    "\n",
    "\n",
    "# create a graph of your classifier's performance on the dev set as a function of the amount of training data\n",
    "# the x-axis should be the amount of training data (as a percentage of the total training data)\n",
    "# NOTE : make sure one of your experiments uses 10% of the data, you will need this to answer the first question in task 5\n",
    "# the y-axis should be the performance of the classifier on the dev set\n",
    "# the graph should have 4 lines, one for each of precision, recall, f1, and accuracy\n",
    "# the graph should have a legend, title, and axis labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your model using both a __binarized__ (bag of words representation where we put 1 [true] if the word is there and 0 [false] otherwise) and a __multinomial__ (bag of words representation where we put the count of the word if the word occurs, and 0 otherwise). Use whichever one gives you a better final f1 score on the dev set to produce your graphs.\n",
    "\n",
    "- f1 score binarized: 0.7761\n",
    "- f1 score multinomial: 0.7813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
